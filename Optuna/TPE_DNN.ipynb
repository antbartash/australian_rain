{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0109817-4a15-4c52-abca-2093461c8943",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf52700f-aa14-499e-8d74-bee5d55fccb3",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T08:08:24.937171Z",
     "iopub.status.busy": "2023-12-29T08:08:24.936820Z",
     "iopub.status.idle": "2023-12-29T08:08:24.942022Z",
     "shell.execute_reply": "2023-12-29T08:08:24.940957Z",
     "shell.execute_reply.started": "2023-12-29T08:08:24.937137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import optuna\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(234)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d0d529-5b4c-464a-a270-9a5bf52cc410",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcf1c2a-bb41-4f1f-a53d-3e78f638f346",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2315fd08-3627-4cf5-b5b8-323c9c3cca47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_dir = ''\n",
    "    X_train = torch.load(os.path.join(data_dir, 'X_train.pt')).detach().numpy()\n",
    "    X_valid = torch.load(os.path.join(data_dir, 'X_valid.pt')).detach().numpy()\n",
    "    X_test = torch.load(os.path.join(data_dir, 'X_test.pt')).detach().numpy()\n",
    "    y_train = torch.load(os.path.join(data_dir, 'y_train.pt')).to(torch.float64).detach().numpy()\n",
    "    y_valid = torch.load(os.path.join(data_dir, 'y_valid.pt')).to(torch.float64).detach().numpy()\n",
    "    y_test = torch.load(os.path.join(data_dir, 'y_test.pt')).to(torch.float64).detach().numpy()\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef760c14-f749-41f9-9b7f-8700401de194",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f16e45c-b1b8-4462-8855-7e31c4eaabb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_model(trial, input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # layer 0\n",
    "    n_units_0 = trial.suggest_int('nunints_layer_0', 128, 2048, step=128)\n",
    "    actv_func_0 = trial.suggest_categorical('actv_func_layer_0', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n",
    "\n",
    "    bias_reg_init_0 = trial.suggest_categorical('bias_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n",
    "    if bias_reg_init_0 == 'l1':\n",
    "        bias_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n",
    "    elif bias_reg_init_0 == 'l2':\n",
    "        bias_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('bias_reg_layer_0', 0.0001, 0.5))\n",
    "    elif bias_reg_init_0 == 'l1l2':\n",
    "        bias_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('bias_reg_layer_0_l1', 0.0001, 0.5),\n",
    "                                                trial.suggest_float('bias_reg_layer_0_l2', 0.0001, 0.5))\n",
    "    else:\n",
    "        bias_reg_0 = None\n",
    "\n",
    "    kernel_reg_init_0 = trial.suggest_categorical('kernel_reg_init_layer_0', ['l1', 'l2', 'l1l2', None])\n",
    "    if kernel_reg_init_0 == 'l1':\n",
    "        kernel_reg_0 = tf.keras.regularizers.l1(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n",
    "    elif kernel_reg_init_0 == 'l2':\n",
    "        kernel_reg_0 = tf.keras.regularizers.l2(trial.suggest_float('kernel_reg_layer_0', 0.0001, 0.5))\n",
    "    elif kernel_reg_init_0 == 'l1l2':\n",
    "        kernel_reg_0 = tf.keras.regularizers.L1L2(trial.suggest_float('kernel_reg_layer_0_l1', 0.0001, 0.5),\n",
    "                                                  trial.suggest_float('kernel_reg_layer_0_l2', 0.0001, 0.5))\n",
    "    else:\n",
    "        kernel_reg_0 = None\n",
    "\n",
    "    kernel_initializer_0 = trial.suggest_categorical(\n",
    "        'kernel_initializer_layer_0', ['glorot_uniform', 'glorot_normal',\n",
    "                                       'he_uniform', 'he_normal',\n",
    "                                       'lecun_uniform', 'lecun_normal']\n",
    "    )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "                                units=n_units_0, activation=actv_func_0, \n",
    "                                kernel_initializer=kernel_initializer_0,\n",
    "                                bias_regularizer=bias_reg_0, kernel_regularizer=kernel_reg_0,\n",
    "                                input_shape=input_shape))\n",
    "\n",
    "    # hidden layers\n",
    "    num_layers = trial.suggest_int('num_layers', 0, 4)\n",
    "    batch_norm = trial.suggest_categorical(f'batch_norm', [True, False])\n",
    "    for layer_num in range(num_layers):\n",
    "        layer_i = layer_num + 1\n",
    "        n_units = trial.suggest_int(f'nunits_layer_{layer_i}', 128, 2048, step=128)\n",
    "        actv_func = trial.suggest_categorical(f'actv_func_layer_{layer_i}', ['relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh', 'gelu'])\n",
    "\n",
    "        bias_reg_init = trial.suggest_categorical(f'bias_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n",
    "        if bias_reg_init == 'l1':\n",
    "            bias_reg = tf.keras.regularizers.l1(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n",
    "        elif bias_reg_init == 'l2':\n",
    "            bias_reg = tf.keras.regularizers.l2(trial.suggest_float(f'bias_reg_layer_{layer_i}', 0.0001, 0.5))\n",
    "        elif bias_reg_init == 'l1l2':\n",
    "            bias_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'bias_reg_layer_{layer_i}_l1', 0.0001, 0.5),\n",
    "                                                  trial.suggest_float(f'bias_reg_layer_{layer_i}_l2', 0.0001, 0.5))\n",
    "        else:\n",
    "            bias_reg = None\n",
    "\n",
    "        kernel_reg_init = trial.suggest_categorical(f'kernel_reg_init_layer_{layer_i}', ['l1', 'l2', 'l1l2', None])\n",
    "        if kernel_reg_init == 'l1':\n",
    "            kernel_reg = tf.keras.regularizers.l1(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.001, 0.5))\n",
    "        elif kernel_reg_init == 'l2':\n",
    "            kernel_reg = tf.keras.regularizers.l2(trial.suggest_float(f'kernel_reg_layer_{layer_i}', 0.001, 0.5))\n",
    "        elif kernel_reg_init == 'l1l2':\n",
    "            kernel_reg = tf.keras.regularizers.L1L2(trial.suggest_float(f'kernel_reg_layer_{layer_i}_l1', 0.001, 0.5),\n",
    "                                                    trial.suggest_float(f'kernel_reg_layer_{layer_i}_l2', 0.001, 0.5))\n",
    "        else:\n",
    "            kernel_reg = None\n",
    "\n",
    "        kernel_initializer = trial.suggest_categorical(\n",
    "            f'kernel_initializer_layer_{layer_i}', ['glorot_uniform', 'glorot_normal',\n",
    "                                                    'he_uniform', 'he_normal',\n",
    "                                                    'lecun_uniform', 'lecun_normal']\n",
    "        )\n",
    "        \n",
    "        if batch_norm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        dropout_rate = trial.suggest_float(f'dropout_rate_layer_{layer_i}', 0.0, 0.999)\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "        model.add(tf.keras.layers.Dense(n_units, actv_func, \n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    bias_regularizer=bias_reg, kernel_regularizer=kernel_reg))\n",
    "    if batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588ecabb-b29d-4b72-8334-60787dc89ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_optimizer(trial):\n",
    "    opt_kwargs = {}\n",
    "    opt_init = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'Nadam', 'Adamax'])\n",
    "    if opt_init == 'SGD':\n",
    "        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-5, 1e-1, log=True)\n",
    "        opt_kwargs['momentum'] = trial.suggest_float('opt_momentum', 1e-5, 0.1, log=True)\n",
    "        opt_kwargs['nesterov'] = trial.suggest_categorical('opt_nesterov', [True, False])\n",
    "    if opt_init == 'Adam':\n",
    "        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-5, 1e-1, log=True)\n",
    "    if opt_init == 'Nadam':\n",
    "        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-5, 1e-1, log=True)\n",
    "    if opt_init == 'Adamax':\n",
    "        opt_kwargs['learning_rate'] = trial.suggest_float('opt_lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(tf.optimizers, opt_init)(**opt_kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859a5e4c-9620-4da8-b105-5ed3b2172868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Objective / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5cf70c-4f37-4ca4-9964-01f61d85e4d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    X_train, X_valid, _, y_train, y_valid, _ = load_data()\n",
    "\n",
    "    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512, 1024])\n",
    "    PREFETCH = 1\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(PREFETCH)\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(BATCH_SIZE).prefetch(PREFETCH)\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(PREFETCH)\n",
    "\n",
    "    model = create_model(trial, X_train.shape[1:])\n",
    "    optimizer = create_optimizer(trial)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=optimizer,\n",
    "                #   metrics=[tf.keras.metrics.F1Score(average='micro')]\n",
    "                metrics=[F1Score(average='micro')]\n",
    "                )\n",
    "    \n",
    "    # callbacks\n",
    "    logdir = os.path.join(\"logs/optuna\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=25, mode='max')\n",
    "    lr_scheduler_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_f1_score', patience=15, mode='max',\n",
    "                                                                 factor=trial.suggest_float('lr_scheduler_factor', 0.1, 0.75))\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=500, \n",
    "                        validation_data=valid_dataset,\n",
    "                        callbacks=[tensorboard_callback, lr_scheduler_callback, earlystopping_callback],\n",
    "                        verbose=0)\n",
    "    print('\\n')\n",
    "    return np.max(history.history['val_f1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7855ceb7-6be6-4599-964c-ab41cd9f0383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7159d99b-8e8b-47e8-a181-9ca2ec3c2605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(\n",
    "    n_startup_trials=40, n_ei_candidates=48,\n",
    "    multivariate=False, seed=42\n",
    ")\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler, study_name='study', storage='sqlite:///db.sqlite3')\n",
    "study.optimize(\n",
    "    objective, n_trials=1000,\n",
    "    timeout=24*3600, # in seconds\n",
    "    n_jobs=1,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef26b003-30be-4dd1-a200-5f2aeebad3e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ba5629-ebe4-4610-943b-a420db7d197a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db3ed809-9223-4c88-9ac3-e8b6d6e06e32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study, params=['lr_scheduler_factor', 'opt_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.params)\n",
    "print(study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial.params"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "optuna",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
